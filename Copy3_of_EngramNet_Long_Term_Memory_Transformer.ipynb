{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/weagan/Engram/blob/main/Copy3_of_EngramNet_Long_Term_Memory_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "improved-intro"
      },
      "source": [
        "# Engram Memory Module: True Performance Demonstration\n",
        "\n",
        "**Key insight**: The Engram module provides explicit, large-capacity memory storage. We need to demonstrate this with a task that:\n",
        "1. Requires remembering specific associations beyond context window\n",
        "2. Benefits from O(1) memory lookup\n",
        "3. Shows the advantage when standard attention would struggle\n",
        "\n",
        "We'll use a **Long-Term Associative Memory Task** where models must remember random facts presented much earlier in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "improved-imports",
        "outputId": "e8bc4c5b-816a-4ed7-9e83-1f71b31b6816",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "improved-engram"
      },
      "source": [
        "## 1. Enhanced Engram Module with Better Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enhanced-engram"
      },
      "outputs": [],
      "source": [
        "class EnhancedEngramModule(nn.Module):\n",
        "    \"\"\"Enhanced Engram with proper initialization and content-based addressing\"\"\"\n",
        "    def __init__(self, table_size=100000, d_model=512, n_heads=4, init_scale=0.02):\n",
        "        super().__init__()\n",
        "        self.table_size = table_size\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        # Initialize memory table with small values (not random)\n",
        "        self.memory_table = nn.Parameter(torch.zeros(table_size, d_model))\n",
        "        nn.init.normal_(self.memory_table, mean=0.0, std=init_scale)\n",
        "\n",
        "        # Content-based addressing (optional - makes it more powerful)\n",
        "        self.query_proj = nn.Linear(d_model, d_model)\n",
        "        self.key_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Gating mechanism\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(d_model * 2, d_model),  # Hidden + retrieved\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Merge projection\n",
        "        self.merge_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def multi_head_hash(self, input_ids):\n",
        "        \"\"\"Deterministic hashing for memory indices\"\"\"\n",
        "        hashes = []\n",
        "        for i in range(self.n_heads):\n",
        "            # Different prime multipliers for each head\n",
        "            prime = [17, 31, 53, 79, 107, 131, 157, 181][i % 8]\n",
        "            hash_val = (input_ids * prime) % self.table_size\n",
        "            hashes.append(hash_val)\n",
        "        return torch.stack(hashes, dim=-1)\n",
        "\n",
        "    def forward(self, hidden_states, input_ids, use_content_addressing=False, debug_print=False):\n",
        "        batch_size, seq_len, _ = hidden_states.shape\n",
        "\n",
        "        # Get indices using hashing\n",
        "        indices = self.multi_head_hash(input_ids)  # [B, S, n_heads]\n",
        "\n",
        "        # Retrieve from memory\n",
        "        retrieved_mem = F.embedding(indices, self.memory_table)  # [B, S, n_heads, d_model]\n",
        "\n",
        "        # Content-based addressing (optional enhancement)\n",
        "        if use_content_addressing:\n",
        "            queries = self.query_proj(hidden_states).unsqueeze(2)  # [B, S, 1, d_model]\n",
        "            keys = self.key_proj(retrieved_mem)  # [B, S, n_heads, d_model]\n",
        "            attention_scores = torch.matmul(queries, keys.transpose(-1, -2))  # [B, S, 1, n_heads]\n",
        "            attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "            retrieved_mem = torch.sum(attention_weights * retrieved_mem, dim=2)  # [B, S, d_model]\n",
        "        else:\n",
        "            # Simple mean pooling\n",
        "            retrieved_mem = retrieved_mem.mean(dim=2)\n",
        "\n",
        "        # Adaptive gating\n",
        "        gate_input = torch.cat([hidden_states, retrieved_mem], dim=-1)\n",
        "        gate_score = self.gate(gate_input) # [B, S, 1]\n",
        "        gated_memory = retrieved_mem * gate_score\n",
        "\n",
        "        # Residual connection\n",
        "        output = hidden_states + self.merge_proj(gated_memory)\n",
        "\n",
        "        # Only print debug info if debug_print is True AND the gate score is low\n",
        "        if debug_print and gate_score.mean().item() < 0.1: # Threshold set to 0.1\n",
        "            print(f\"\\n--- Engram Debug Info (Gate < 0.1) ---\")\n",
        "            print(f\"  Input IDs (sample): {input_ids[0, :5]}\")\n",
        "            print(f\"  Hashing Indices (sample): {indices[0, :5, :].cpu().numpy()}\")\n",
        "            print(f\"  Memory Table (mean, std): {self.memory_table.mean().item():.4f}, {self.memory_table.std().item():.4f}\")\n",
        "            print(f\"  Retrieved Memory (mean, std): {retrieved_mem.mean().item():.4f}, {retrieved_mem.std().item():.4f}\")\n",
        "            print(f\"  Gate Score (sample, mean): {gate_score[0, :5].squeeze().detach().cpu().numpy()}, {gate_score.mean().item():.4f}\")\n",
        "            print(f\"-------------------------\")\n",
        "        return output, gate_score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78fb8f56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd0dfa14-81a0-4258-d49c-e773bb91901c"
      },
      "source": [
        "engram_module_test = EnhancedEngramModule(table_size=100000, d_model=512, n_heads=4)\n",
        "num_engram_params = sum(p.numel() for p in engram_module_test.parameters() if p.requires_grad)\n",
        "print(f\"Number of parameters in EnhancedEngramModule: {num_engram_params:,}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters in EnhancedEngramModule: 52,513,281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "challenging-task"
      },
      "source": [
        "## 2. Properly Challenging Task: Long-Term Fact Retention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4820b92"
      },
      "source": [
        "class LongTermMemoryTask:\n",
        "    \"\"\"A task that truly tests long-term memory capabilities\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size=5000, max_seq_len=512, num_facts=100, fact_length=3):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.num_facts = num_facts\n",
        "        self.fact_length = fact_length\n",
        "\n",
        "        # Generate random facts: (trigger word, fact words...)\n",
        "        self.facts = []\n",
        "        for i in range(num_facts):\n",
        "            trigger = 1000 + i  # Reserve space for triggers\n",
        "            fact_words = list(np.random.randint(2000, vocab_size-100, size=fact_length))\n",
        "            self.facts.append((trigger, fact_words))\n",
        "\n",
        "        print(f\"Generated {num_facts} facts to remember\")\n",
        "        print(f\"Each fact: trigger word -> {fact_length} fact words\")\n",
        "\n",
        "    def generate_example(self, num_facts_in_sequence=10, distraction_length=200, non_trigger_ratio=0.5):\n",
        "        \"\"\"Generate a single example with long-term memory test\"\"\"\n",
        "        # Select random facts to include\n",
        "        selected_indices = np.random.choice(len(self.facts), num_facts_in_sequence, replace=False)\n",
        "        selected_facts = [self.facts[i] for i in selected_indices]\n",
        "\n",
        "        # Build sequence\n",
        "        sequence = [0]  # BOS\n",
        "        targets = [-100]\n",
        "        fact_positions = []  # Store where triggers are\n",
        "\n",
        "        # Phase 1: Present facts to remember\n",
        "        for trigger, fact_words in selected_facts:\n",
        "            # Store trigger position\n",
        "            fact_positions.append(len(sequence))\n",
        "\n",
        "            # Add trigger and fact\n",
        "            sequence.append(trigger)\n",
        "            targets.append(-100)\n",
        "\n",
        "            sequence.extend(fact_words)\n",
        "            targets.extend([-100] * len(fact_words))\n",
        "\n",
        "        # Phase 2: Long distraction (models must retain facts)\n",
        "        sequence.append(1)  # SEP token\n",
        "        targets.append(-100)\n",
        "\n",
        "        distraction = list(np.random.randint(10, 1000, size=distraction_length))\n",
        "        sequence.extend(distraction)\n",
        "        targets.extend([-100] * distraction_length)\n",
        "\n",
        "        # Phase 3: Test recall with triggers and non-trigger words\n",
        "        sequence.append(2)  # SEP token\n",
        "        targets.append(-100)\n",
        "\n",
        "        test_triggers = []\n",
        "        expected_outputs = []\n",
        "\n",
        "        for i, (trigger, fact_words) in enumerate(selected_facts):\n",
        "            # Only test some facts (50%)\n",
        "            if np.random.random() > 0.5:\n",
        "                sequence.append(trigger)\n",
        "                targets.append(fact_words[0])  # First fact word\n",
        "                test_triggers.append(trigger)\n",
        "                expected_outputs.append(fact_words[0])\n",
        "\n",
        "            # Randomly include non-trigger words during the test phase\n",
        "            if np.random.random() < non_trigger_ratio:\n",
        "                # Add a non-trigger word (from distraction range)\n",
        "                non_trigger_word = np.random.randint(10, 1000)\n",
        "                sequence.append(non_trigger_word)\n",
        "                targets.append(3) # Expected target is padding token (3)\n",
        "\n",
        "        # Pad to max length\n",
        "        while len(sequence) < self.max_seq_len:\n",
        "            sequence.append(3)  # PAD\n",
        "            targets.append(-100)\n",
        "\n",
        "        return {\n",
        "            'sequence': torch.tensor(sequence[:self.max_seq_len]),\n",
        "            'targets': torch.tensor(targets[:self.max_seq_len]),\n",
        "            'num_facts_presented': len(selected_facts),\n",
        "            'num_facts_tested': len(test_triggers),\n",
        "            'distraction_length': distraction_length\n",
        "        }\n",
        "\n",
        "    def generate_batch(self, batch_size=8, non_trigger_ratio=0.5):\n",
        "        \"\"\"Generate batch of examples\"\"\"\n",
        "        sequences = []\n",
        "        targets = []\n",
        "\n",
        "        for _ in range(batch_size):\n",
        "            example = self.generate_example(\n",
        "                num_facts_in_sequence=np.random.randint(5, 15),\n",
        "                distraction_length=np.random.randint(300, 500), # Increased distraction length\n",
        "                non_trigger_ratio=non_trigger_ratio\n",
        "            )\n",
        "            sequences.append(example['sequence'])\n",
        "            targets.append(example['targets'])\n",
        "\n",
        "        return torch.stack(sequences), torch.stack(targets)\n",
        "\n",
        "    def calculate_memory_accuracy(self, logits, targets):\n",
        "        \"\"\"Calculate accuracy on memory test positions, distinguishing facts and non-triggers\"\"\"\n",
        "        predictions = logits.argmax(dim=-1)\n",
        "\n",
        "        # Mask for actual fact recall positions (targets are fact words)\n",
        "        # Target is not -100 (ignored), not 0,1,2 (special tokens), and not 3 (padding/non-trigger)\n",
        "        fact_mask = (targets != -100) & (targets != 0) & (targets != 1) & (targets != 2) & (targets != 3)\n",
        "\n",
        "        # Mask for non-trigger word positions (targets are padding token 3)\n",
        "        nontrigger_mask = (targets == 3) # where targets are explicitly set to PAD token 3 for non-triggers\n",
        "\n",
        "        fact_accuracy = 0.0\n",
        "        if fact_mask.sum() > 0:\n",
        "            correct_facts = (predictions[fact_mask] == targets[fact_mask]).float().sum()\n",
        "            fact_accuracy = (correct_facts / fact_mask.sum()).item()\n",
        "\n",
        "        nontrigger_accuracy = 0.0\n",
        "        if nontrigger_mask.sum() > 0:\n",
        "            correct_nontriggers = (predictions[nontrigger_mask] == targets[nontrigger_mask]).float().sum()\n",
        "            nontrigger_accuracy = (correct_nontriggers / nontrigger_mask.sum()).item()\n",
        "\n",
        "        # Overall accuracy for relevant positions\n",
        "        combined_mask = fact_mask | nontrigger_mask\n",
        "        overall_accuracy = 0.0\n",
        "        if combined_mask.sum() > 0:\n",
        "            correct_overall = (predictions[combined_mask] == targets[combined_mask]).float().sum()\n",
        "            overall_accuracy = (correct_overall / combined_mask.sum()).item()\n",
        "\n",
        "        return {\n",
        "            'fact_accuracy': fact_accuracy,\n",
        "            'nontrigger_accuracy': nontrigger_accuracy,\n",
        "            'overall_accuracy': overall_accuracy\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class LongTermMemoryTask:\n",
        "    \"\"\"A task that truly tests long-term memory capabilities\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size=5000, max_seq_len=512, num_facts=100, fact_length=3):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.num_facts = num_facts\n",
        "        self.fact_length = fact_length\n",
        "\n",
        "        # Generate random facts: (trigger word, fact words...)\n",
        "        self.facts = []\n",
        "        for i in range(num_facts):\n",
        "            trigger = 1000 + i  # Reserve space for triggers\n",
        "            fact_words = list(np.random.randint(2000, vocab_size-100, size=fact_length))\n",
        "            self.facts.append((trigger, fact_words))\n",
        "\n",
        "        print(f\"Generated {num_facts} facts to remember\")\n",
        "        print(f\"Each fact: trigger word -> {fact_length} fact words\")\n",
        "\n",
        "    def generate_example(self, num_facts_in_sequence=10, distraction_length=200, non_trigger_ratio=0.5):\n",
        "        \"\"\"Generate a single example with long-term memory test\"\"\"\n",
        "        # Select random facts to include\n",
        "        selected_indices = np.random.choice(len(self.facts), num_facts_in_sequence, replace=False)\n",
        "        selected_facts = [self.facts[i] for i in selected_indices]\n",
        "\n",
        "        # Build sequence\n",
        "        sequence = [0]  # BOS\n",
        "        targets = [-100]\n",
        "        fact_positions = []  # Store where triggers are\n",
        "\n",
        "        # Phase 1: Present facts to remember\n",
        "        for trigger, fact_words in selected_facts:\n",
        "            # Store trigger position\n",
        "            fact_positions.append(len(sequence))\n",
        "\n",
        "            # Add trigger and fact\n",
        "            sequence.append(trigger)\n",
        "            targets.append(-100)\n",
        "\n",
        "            sequence.extend(fact_words)\n",
        "            targets.extend([-100] * len(fact_words))\n",
        "\n",
        "        # Phase 2: Long distraction (models must retain facts)\n",
        "        sequence.append(1)  # SEP token\n",
        "        targets.append(-100)\n",
        "\n",
        "        distraction = list(np.random.randint(10, 1000, size=distraction_length))\n",
        "        sequence.extend(distraction)\n",
        "        targets.extend([-100] * distraction_length)\n",
        "\n",
        "        # Phase 3: Test recall with triggers and non-trigger words\n",
        "        sequence.append(2)  # SEP token\n",
        "        targets.append(-100)\n",
        "\n",
        "        test_triggers = []\n",
        "        expected_outputs = []\n",
        "\n",
        "        for i, (trigger, fact_words) in enumerate(selected_facts):\n",
        "            # Only test some facts (50%)\n",
        "            if np.random.random() > 0.5:\n",
        "                sequence.append(trigger)\n",
        "                targets.append(fact_words[0])  # First fact word\n",
        "                test_triggers.append(trigger)\n",
        "                expected_outputs.append(fact_words[0])\n",
        "\n",
        "            # Randomly include non-trigger words during the test phase\n",
        "            if np.random.random() < non_trigger_ratio:\n",
        "                # Add a non-trigger word (from distraction range)\n",
        "                non_trigger_word = np.random.randint(10, 1000)\n",
        "                sequence.append(non_trigger_word)\n",
        "                targets.append(3) # Expected target is padding token (3)\n",
        "\n",
        "        # Pad to max length\n",
        "        while len(sequence) < self.max_seq_len:\n",
        "            sequence.append(3)  # PAD\n",
        "            targets.append(-100)\n",
        "\n",
        "        return {\n",
        "            'sequence': torch.tensor(sequence[:self.max_seq_len]),\n",
        "            'targets': torch.tensor(targets[:self.max_seq_len]),\n",
        "            'num_facts_presented': len(selected_facts),\n",
        "            'num_facts_tested': len(test_triggers),\n",
        "            'distraction_length': distraction_length\n",
        "        }\n",
        "\n",
        "    def generate_batch(self, batch_size=8, non_trigger_ratio=0.5):\n",
        "        \"\"\"Generate batch of examples\"\"\"\n",
        "        sequences = []\n",
        "        targets = []\n",
        "\n",
        "        for _ in range(batch_size):\n",
        "            example = self.generate_example(\n",
        "                num_facts_in_sequence=np.random.randint(5, 15),\n",
        "                distraction_length=np.random.randint(300, 500), # Increased distraction length\n",
        "                non_trigger_ratio=non_trigger_ratio\n",
        "            )\n",
        "            sequences.append(example['sequence'])\n",
        "            targets.append(example['targets'])\n",
        "\n",
        "        return torch.stack(sequences), torch.stack(targets)\n",
        "\n",
        "    def calculate_memory_accuracy(self, logits, targets):\n",
        "        \"\"\"Calculate accuracy on memory test positions, distinguishing facts and non-triggers\"\"\"\n",
        "        predictions = logits.argmax(dim=-1)\n",
        "\n",
        "        # Mask for actual fact recall positions (targets are fact words)\n",
        "        # Target is not -100 (ignored), not 0,1,2 (special tokens), and not 3 (padding/non-trigger)\n",
        "        fact_mask = (targets != -100) & (targets != 0) & (targets != 1) & (targets != 2) & (targets != 3)\n",
        "\n",
        "        # Mask for non-trigger word positions (targets are padding token 3)\n",
        "        nontrigger_mask = (targets == 3) # where targets are explicitly set to PAD token 3 for non-triggers\n",
        "\n",
        "        fact_accuracy = 0.0\n",
        "        if fact_mask.sum() > 0:\n",
        "            correct_facts = (predictions[fact_mask] == targets[fact_mask]).float().sum()\n",
        "            fact_accuracy = (correct_facts / fact_mask.sum()).item()\n",
        "\n",
        "        nontrigger_accuracy = 0.0\n",
        "        if nontrigger_mask.sum() > 0:\n",
        "            correct_nontriggers = (predictions[nontrigger_mask] == targets[nontrigger_mask]).float().sum()\n",
        "            nontrigger_accuracy = (correct_nontriggers / nontrigger_mask.sum()).item()\n",
        "\n",
        "        # Overall accuracy for relevant positions\n",
        "        combined_mask = fact_mask | nontrigger_mask\n",
        "        overall_accuracy = 0.0\n",
        "        if combined_mask.sum() > 0:\n",
        "            correct_overall = (predictions[combined_mask] == targets[combined_mask]).float().sum()\n",
        "            overall_accuracy = (correct_overall / combined_mask.sum()).item()\n",
        "\n",
        "        return {\n",
        "            'fact_accuracy': fact_accuracy,\n",
        "            'nontrigger_accuracy': nontrigger_accuracy,\n",
        "            'overall_accuracy': overall_accuracy\n",
        "        }\n"
      ],
      "metadata": {
        "id": "6a1a4c59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "class LongTermMemoryTask:\n",
        "    \"\"\"A task that truly tests long-term memory capabilities\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size=5000, max_seq_len=512, num_facts=100, fact_length=3):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.num_facts = num_facts\n",
        "        self.fact_length = fact_length\n",
        "\n",
        "        # Generate random facts: (trigger word, fact words...)\n",
        "        self.facts = []\n",
        "        for i in range(num_facts):\n",
        "            trigger = 1000 + i  # Reserve space for triggers\n",
        "            fact_words = list(np.random.randint(2000, vocab_size-100, size=fact_length))\n",
        "            self.facts.append((trigger, fact_words))\n",
        "\n",
        "        print(f\"Generated {num_facts} facts to remember\")\n",
        "        print(f\"Each fact: trigger word -> {fact_length} fact words\")\n",
        "\n",
        "    def generate_example(self, num_facts_in_sequence=10, distraction_length=200):\n",
        "        \"\"\"Generate a single example with long-term memory test\"\"\"\n",
        "        # Select random facts to include\n",
        "        selected_indices = np.random.choice(len(self.facts), num_facts_in_sequence, replace=False)\n",
        "        selected_facts = [self.facts[i] for i in selected_indices]\n",
        "\n",
        "        # Build sequence\n",
        "        sequence = [0]  # BOS\n",
        "        targets = [-100]\n",
        "        fact_positions = []  # Store where triggers are\n",
        "\n",
        "        # Phase 1: Present facts to remember\n",
        "        for trigger, fact_words in selected_facts:\n",
        "            # Store trigger position\n",
        "            fact_positions.append(len(sequence))\n",
        "\n",
        "            # Add trigger and fact\n",
        "            sequence.append(trigger)\n",
        "            targets.append(-100)\n",
        "\n",
        "            sequence.extend(fact_words)\n",
        "            targets.extend([-100] * len(fact_words))\n",
        "\n",
        "        # Phase 2: Long distraction (models must retain facts)\n",
        "        sequence.append(1)  # SEP token\n",
        "        targets.append(-100)\n",
        "\n",
        "        distraction = list(np.random.randint(10, 1000, size=distraction_length))\n",
        "        sequence.extend(distraction)\n",
        "        targets.extend([-100] * distraction_length)\n",
        "\n",
        "        # Phase 3: Test recall with triggers only\n",
        "        sequence.append(2)  # SEP token\n",
        "        targets.append(-100)\n",
        "\n",
        "        test_triggers = []\n",
        "        expected_outputs = []\n",
        "\n",
        "        for i, (trigger, fact_words) in enumerate(selected_facts):\n",
        "            # Only test some facts (50%)\n",
        "            if np.random.random() > 0.5:\n",
        "                sequence.append(trigger)\n",
        "                targets.append(fact_words[0])  # First fact word\n",
        "                test_triggers.append(trigger)\n",
        "                expected_outputs.append(fact_words[0])\n",
        "\n",
        "        # Pad to max length\n",
        "        while len(sequence) < self.max_seq_len:\n",
        "            sequence.append(3)  # PAD\n",
        "            targets.append(-100)\n",
        "\n",
        "        return {\n",
        "            'sequence': torch.tensor(sequence[:self.max_seq_len]),\n",
        "            'targets': torch.tensor(targets[:self.max_seq_len]),\n",
        "            'num_facts_presented': len(selected_facts),\n",
        "            'num_facts_tested': len(test_triggers),\n",
        "            'distraction_length': distraction_length\n",
        "        }\n",
        "\n",
        "    def generate_batch(self, batch_size=8):\n",
        "        \"\"\"Generate batch of examples\"\"\"\n",
        "        sequences = []\n",
        "        targets = []\n",
        "\n",
        "        for _ in range(batch_size):\n",
        "            example = self.generate_example(\n",
        "                num_facts_in_sequence=np.random.randint(5, 15),\n",
        "                distraction_length=np.random.randint(300, 500) # Increased distraction length\n",
        "            )\n",
        "            sequences.append(example['sequence'])\n",
        "            targets.append(example['targets'])\n",
        "\n",
        "        return torch.stack(sequences), torch.stack(targets)\n",
        "\n",
        "    def calculate_memory_accuracy(self, logits, targets):\n",
        "        \"\"\"Calculate accuracy only on memory test positions\"\"\"\n",
        "        mask = (targets != -100) & (targets != 0) & (targets != 1) & (targets != 2) & (targets != 3)\n",
        "        if mask.sum() == 0:\n",
        "            return 0.0\n",
        "\n",
        "        predictions = logits.argmax(dim=-1)\n",
        "        correct = (predictions[mask] == targets[mask]).float().sum()\n",
        "\n",
        "        return (correct / mask.sum()).item()"
      ],
      "metadata": {
        "id": "long-term-task"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aaa195ef",
        "outputId": "b9f23f64-3642-45e3-b0d9-3b51a9fca49b"
      },
      "source": [
        "task_example = LongTermMemoryTask(vocab_size=5000, max_seq_len=512, num_facts=10, fact_length=3)\n",
        "example_data = task_example.generate_example(\n",
        "    num_facts_in_sequence=3, # Fewer facts for a clearer example\n",
        "    distraction_length=50    # Shorter distraction for a clearer example\n",
        ")\n",
        "\n",
        "print(\"--- Example Data from generate_example ---\")\n",
        "print(f\"Sequence (first 20 tokens): {example_data['sequence'][:20].tolist()}...\")\n",
        "print(f\"Targets (first 20 tokens):  {example_data['targets'][:20].tolist()}...\")\n",
        "print(f\"Number of facts presented: {example_data['num_facts_presented']}\")\n",
        "print(f\"Number of facts tested: {example_data['num_facts_tested']}\")\n",
        "print(f\"Distraction length: {example_data['distraction_length']}\")\n",
        "\n",
        "# Also print some parts of the full sequence and targets for more context\n",
        "print(\"\\n--- Full Sequence and Targets Info ---\")\n",
        "print(f\"Full Sequence Length: {len(example_data['sequence'])}\")\n",
        "print(f\"Full Targets Length: {len(example_data['targets'])}\")\n",
        "\n",
        "# Displaying the raw tensors for completeness, trimming if too long\n",
        "print(\"\\nRaw Sequence Tensor:\")\n",
        "display(example_data['sequence'])\n",
        "print(\"\\nRaw Targets Tensor:\")\n",
        "display(example_data['targets'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 10 facts to remember\n",
            "Each fact: trigger word -> 3 fact words\n",
            "--- Example Data from generate_example ---\n",
            "Sequence (first 20 tokens): [0, 1001, 3095, 3638, 4169, 1008, 4300, 2747, 2474, 1007, 3184, 2459, 2021, 1, 850, 176, 283, 397, 610, 325]...\n",
            "Targets (first 20 tokens):  [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]...\n",
            "Number of facts presented: 3\n",
            "Number of facts tested: 2\n",
            "Distraction length: 50\n",
            "\n",
            "--- Full Sequence and Targets Info ---\n",
            "Full Sequence Length: 512\n",
            "Full Targets Length: 512\n",
            "\n",
            "Raw Sequence Tensor:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([   0, 1001, 3095, 3638, 4169, 1008, 4300, 2747, 2474, 1007, 3184, 2459,\n",
              "        2021,    1,  850,  176,  283,  397,  610,  325,   23,  251,  786,  355,\n",
              "         574,  907,  349,  101,  376,  965,  464,  437,  518,  785,  952,   44,\n",
              "         215,   90,  941,  571,  881,  397,   11,  399,  575,  115,  781,  831,\n",
              "         486,  712,  411,  739,  565,  171,  211,  967,  279,  872,  825,  280,\n",
              "         465,  471,  736,  261,    2,  729, 1008, 1007,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
              "           3,    3,    3,    3,    3,    3,    3,    3])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Raw Targets Tensor:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100,    3, 4300, 3184, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
              "        -100, -100, -100, -100, -100, -100, -100, -100])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "models-with-differences"
      },
      "source": [
        "## 3. Models with Clear Architectural Differences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.  ### `BaselineTransformerNoMemory`\n",
        "\n",
        "    -   **Purpose**: This model represents a standard Transformer with a **severely limited context window** (`max_context=128`). This limitation is deliberate; it simulates a scenario where a model cannot 'see' far enough back in its input sequence to recall facts presented much earlier.\n",
        "    -   **Architecture**: It has an embedding layer, multiple self-attention layers (`nn.MultiheadAttention`), and Feed-Forward Networks (FFNs). The crucial part is how attention is applied: for sequences longer than `max_context`, it only attends to the most recent `max_context` tokens. This means any fact presented before this window is effectively 'forgotten' by its attention mechanism.\n",
        "    -   **Expected Behavior**: It is *designed to struggle* with the long-term memory task, especially when distractions push the relevant facts outside its limited attention span.\n",
        "2.  ### `EngramEnhancedTransformer`\n",
        "\n",
        "    -   **Purpose**: This is the core model demonstrating the Engram module's utility. It also has a potentially limited context for its *attention* (though in this specific setup, its attention is not explicitly limited beyond the sequence length, the *Engram* module provides the long-term memory). Its key feature is the integration of the `EnhancedEngramModule` to provide explicit, long-term memory.\n",
        "    -   **Architecture**: It starts like a standard Transformer with embeddings, self-attention layers, and FFNs. However, *after* each self-attention block, it incorporates an `EnhancedEngramModule`.\n",
        "        -   **Key difference**: Instead of solely relying on the (limited) attention mechanism for memory, it actively queries its `engram_layers` using the `input_ids`. This allows it to retrieve information from a large, separate memory table regardless of how far back in the sequence the information was first presented. The retrieved memory is then integrated into the model's hidden states.\n",
        "    -   **Expected Behavior**: This model is *designed to excel* at the long-term memory task. Even if the original fact is pushed far outside the attention window by distractions, the Engram module can still retrieve it efficiently via its hashing mechanism.\n",
        "3.  ### `HybridTransformer`\n",
        "\n",
        "    -   **Purpose**: This model serves as a strong baseline comparison, representing a powerful Transformer that *doesn't* have an explicit Engram memory but *does* have a **full attention mechanism** (i.e., it can attend to the entire input sequence without a fixed `max_context` limit).\n",
        "    -   **Architecture**: It's a more traditional Transformer decoder architecture, with embedding, multi-head self-attention, and FFN layers. The attention mechanism can calculate relationships between any token and all preceding tokens in the sequence.\n",
        "    -   **Expected Behavior**: This model is *expected to perform well* on the long-term memory task because its attention can theoretically reach back to any point in the sequence. However, it will be computationally more expensive (O(n²) with respect to sequence length) compared to Engram's O(1) memory access, especially for very long sequences."
      ],
      "metadata": {
        "id": "oiOh1I5Hfo9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "-   The **Baseline** trains fastest because its attention mechanism is explicitly limited to a small context (`max_context=128`), meaning fewer computations per step.\n",
        "-   The **Hybrid** model performs full attention over the entire sequence, which has an O(N²) computational cost, making it slower than the Baseline.\n",
        "-   The **Engram-Enhanced** model performs full attention *and* includes the additional operations for the `EnhancedEngramModule` (retrieval, gating, merging), adding to its computational load per step, making it the slowest to train."
      ],
      "metadata": {
        "id": "ilasj_IPlgFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  ### **BaselineTransformerNoMemory**\n",
        "\n",
        "    -   **Inference Speed**: **Fastest**. This model processes information quickly because its attention mechanism is explicitly limited to a small context window (`max_context=128`).\n",
        "    -   **Accuracy**: **Least reliable for long-term memory**. While its overall reported accuracy might be decent (0.9350), it is fundamentally *designed to struggle* with recalling facts presented outside its narrow context window, especially with long distractions. For a true long-term memory task where information is far removed, its accuracy will plummet.\n",
        "    -   **Conclusion**: Choose if raw speed is paramount and the task's memory requirements are strictly short-term.\n",
        "2.  ### **HybridTransformer (Full Attention)**\n",
        "\n",
        "    -   **Inference Speed**: **Moderate**. It's slower than the Baseline due to its O(N²) computational cost for full attention over the entire sequence. However, it is generally faster than the Engram-Enhanced model.\n",
        "    -   **Accuracy**: **High and generally good across the full sequence**. It achieved an overall accuracy of 0.9500. This model is expected to perform well on long-term memory tasks as long as the entire sequence can fit into memory and computational limits allow for O(N²) attention. Its accuracy comes from its ability to attend to all previous tokens.\n",
        "    -   **Conclusion**: Offers a **good balance of speed and high accuracy** for tasks requiring full-sequence context, provided the sequence length (N) doesn't make O(N²) attention prohibitively expensive.\n",
        "3.  ### **EngramEnhancedTransformer**\n",
        "\n",
        "    -   **Inference Speed**: **Slowest**. It incurs the O(N²) cost of full attention *plus* the additional overhead of the `EnhancedEngramModule` (hashing, memory lookup, gating, merging) in each layer.\n",
        "    -   **Accuracy**: **Highest specifically for true long-term fact retention**. It achieved a perfect 1.0000 on the challenging 300-token distraction test, demonstrating its ability to remember facts regardless of how far back they were presented. Its overall accuracy was 0.9450, slightly below Hybrid, but its strength is in its *robustness* for explicit, long-term recall.\n",
        "    -   **Conclusion**: Choose if **uncompromising accuracy for explicit, long-term factual recall** is the absolute priority, even at the cost of slower inference. It excels where the Hybrid might falter at extremely long sequences or highly specific, isolated fact retrieval due to the implicit nature of attention memory.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "-   If you need **raw speed above all else** and short-term memory is sufficient: **Baseline**.\n",
        "-   If you need **strong accuracy across the full sequence with reasonable speed**: **Hybrid** is often the best general-purpose choice.\n",
        "-   If you need **guaranteed, explicit long-term fact retention** despite very long distractions, and can tolerate slower inference: **Engram-Enhanced** is specialized for this challenge."
      ],
      "metadata": {
        "id": "1wWkO7Oxm9WM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "proper-models",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "0967e703-9586-4760-feb5-295a3b38c756"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1503371273.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBaselineTransformerNoMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Baseline with limited context window to emphasize memory need\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BaselineTransformerNoMemory(nn.Module):\n",
        "    \"\"\"Baseline with limited context window to emphasize memory need\"\"\"\n",
        "    def __init__(self, vocab_size=5000, d_model=256, n_layers=4, n_heads=8, max_context=128):\n",
        "        super().__init__()\n",
        "        self.max_context = max_context\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # Limited context attention\n",
        "        self.attention_layers = nn.ModuleList([\n",
        "            nn.MultiheadAttention(d_model, n_heads, batch_first=True, dropout=0.1)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.ffn_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(d_model, d_model * 4),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(d_model * 4, d_model),\n",
        "                nn.Dropout(0.1)\n",
        "            )\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(n_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(n_layers)])\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids, debug_print=False):\n",
        "        x = self.embedding(input_ids)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply limited context attention\n",
        "        for i in range(len(self.attention_layers)):\n",
        "            seq_len = x.shape[1]\n",
        "            if seq_len > self.max_context:\n",
        "                if debug_print:\n",
        "                    print(f\"\\n--- Baseline Debug Info (Layer {i}) ---\")\n",
        "                    print(f\"  Context limit ({self.max_context}) reached! Attending to last {self.max_context} tokens out of {seq_len}.\")\n",
        "                    print(f\"-------------------------\")\n",
        "                attn_input = x[:, -self.max_context:, :]\n",
        "            else:\n",
        "                attn_input = x\n",
        "\n",
        "            attn_output, _ = self.attention_layers[i](attn_input, attn_input, attn_input)\n",
        "\n",
        "            if seq_len > self.max_context:\n",
        "                # Update only the last max_context tokens\n",
        "                x = torch.cat([\n",
        "                    x[:, :-self.max_context, :],\n",
        "                    self.norms1[i](x[:, -self.max_context:, :] + self.dropout(attn_output))\n",
        "                ], dim=1)\n",
        "            else:\n",
        "                x = self.norms1[i](x + self.dropout(attn_output))\n",
        "\n",
        "            # FFN\n",
        "            ffn_output = self.ffn_layers[i](x)\n",
        "            x = self.norms2[i](x + self.dropout(ffn_output))\n",
        "\n",
        "        logits = self.output_proj(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class EngramEnhancedTransformer(nn.Module):\n",
        "    \"\"\"Transformer with Engram memory - can remember beyond context window\"\"\"\n",
        "    def __init__(self, vocab_size=5000, d_model=256, n_layers=4, n_heads=8, memory_size=50000):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # Standard attention (still limited context)\n",
        "        self.attention_layers = nn.ModuleList([\n",
        "            nn.MultiheadAttention(d_model, n_heads, batch_first=True, dropout=0.1)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # Engram memory modules (one per layer)\n",
        "        self.engram_layers = nn.ModuleList([\n",
        "            EnhancedEngramModule(table_size=memory_size, d_model=d_model, n_heads=4)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.ffn_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(d_model, d_model * 4),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(d_model * 4, d_model),\n",
        "                nn.Dropout(0.1)\n",
        "            )\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(n_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(n_layers)])\n",
        "        self.norms3 = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(n_layers)])  # After engram\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids, debug_print=False):\n",
        "        x = self.embedding(input_ids)\n",
        "        x = self.dropout(x)\n",
        "        last_gate_score = None # To store the gate score from the last layer\n",
        "\n",
        "        for i in range(len(self.attention_layers)):\n",
        "            # Self-attention\n",
        "            attn_output, _ = self.attention_layers[i](x, x, x)\n",
        "            x = self.norms1[i](x + self.dropout(attn_output))\n",
        "\n",
        "            # Engram memory retrieval (key difference!)\n",
        "            # This allows accessing facts presented much earlier\n",
        "            x_engram_output, gate_score = self.engram_layers[i](x, input_ids, debug_print=debug_print) # Now returns gate_score\n",
        "            x = self.norms3[i](x_engram_output) # Apply norm to the tensor output\n",
        "            last_gate_score = gate_score # Update last_gate_score\n",
        "\n",
        "            # FFN\n",
        "            ffn_output = self.ffn_layers[i](x)\n",
        "            x = self.norms2[i](x + self.dropout(ffn_output))\n",
        "\n",
        "        logits = self.output_proj(x)\n",
        "        return logits, last_gate_score # Return gate_score from the last Engram layer\n",
        "\n",
        "\n",
        "class HybridTransformer(nn.Module):\n",
        "    \"\"\"For comparison: Transformer with larger context but no explicit memory\"\"\"\n",
        "    def __init__(self, vocab_size=5000, d_model=256, n_layers=4, n_heads=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # Full attention (no context limit)\n",
        "        self.attention_layers = nn.ModuleList([\n",
        "            nn.MultiheadAttention(d_model, n_heads, batch_first=True, dropout=0.1)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.ffn_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(d_model, d_model * 4),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(d_model * 4, d_model),\n",
        "                nn.Dropout(0.1)\n",
        "            )\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        self.norms1 = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(n_layers)])\n",
        "        self.norms2 = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(n_layers)])\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embedding(input_ids)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(len(self.attention_layers)):\n",
        "            attn_output, _ = self.attention_layers[i](x, x, x)\n",
        "            x = self.norms1[i](x + self.dropout(attn_output))\n",
        "\n",
        "            ffn_output = self.ffn_layers[i](x)\n",
        "            x = self.norms2[i](x + self.dropout(ffn_output))\n",
        "\n",
        "        logits = self.output_proj(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evaluate-performance"
      },
      "source": [
        "def evaluate_memory_performance(model, task, num_tests=50):\n",
        "    \"\"\"Comprehensive evaluation of memory performance\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    metrics = {\n",
        "        'short_term': [],      # Facts with short distraction\n",
        "        'long_term': [],       # Facts with long distraction\n",
        "        'many_facts': [],      # Many facts to remember\n",
        "        'few_facts': []        # Few facts to remember\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_tests):\n",
        "            # Test different scenarios\n",
        "            scenarios = [\n",
        "                {'distraction_length': 50, 'num_facts': 5},\n",
        "                {'distraction_length': 300, 'num_facts': 5},\n",
        "                {'distraction_length': 150, 'num_facts': 15},\n",
        "                {'distraction_length': 150, 'num_facts': 3}\n",
        "            ]\n",
        "\n",
        "            for i, scenario in enumerate(scenarios):\n",
        "                # Generate custom example\n",
        "                example = task.generate_example(\n",
        "                    num_facts_in_sequence=scenario['num_facts'],\n",
        "                    distraction_length=scenario['distraction_length']\n",
        "                )\n",
        "\n",
        "                inputs = example['sequence'].unsqueeze(0).to(device)\n",
        "                targets = example['targets'].unsqueeze(0).to(device)\n",
        "\n",
        "                logits = model(inputs)\n",
        "                acc = task.calculate_memory_accuracy(logits, targets)\n",
        "\n",
        "                # Categorize\n",
        "                if i == 0:\n",
        "                    metrics['short_term'].append(acc)\n",
        "                elif i == 1:\n",
        "                    metrics['long_term'].append(acc)\n",
        "                elif i == 2:\n",
        "                    metrics['many_facts'].append(acc)\n",
        "                else:\n",
        "                    metrics['few_facts'].append(acc)\n",
        "\n",
        "    # Average each category\n",
        "    for key in metrics:\n",
        "        metrics[key] = np.mean(metrics[key])\n",
        "\n",
        "    # Overall score\n",
        "    metrics['overall'] = np.mean(list(metrics.values()))\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training-improved"
      },
      "source": [
        "## 4. Specialized Training with Memory Pre-loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "155e2937"
      },
      "source": [
        "def train_with_memory_focus(model, task, model_name, num_epochs=15, debug_interval={'epoch': 0, 'batch': 0}, non_trigger_ratio=0.5):\n",
        "    \"\"\"Training that emphasizes memory capabilities\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Special optimizer for Engram\n",
        "    if \"Engram\" in model_name:\n",
        "        # Give memory parameters higher learning rate\n",
        "        memory_params = []\n",
        "        other_params = []\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if \"memory_table\" in name:\n",
        "                memory_params.append(param)\n",
        "            else:\n",
        "                other_params.append(param)\n",
        "\n",
        "        optimizer = torch.optim.AdamW([\n",
        "            {'params': memory_params, 'lr': 1e-3},\n",
        "            {'params': other_params, 'lr': 1e-4}\n",
        "        ])\n",
        "    else:\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    # Tracking metrics\n",
        "    train_losses = []\n",
        "    memory_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_mem_acc_fact = 0 # Track fact accuracy\n",
        "        epoch_mem_acc_nontrigger = 0 # Track non-trigger accuracy\n",
        "        epoch_mem_acc_overall = 0 # Track overall accuracy\n",
        "        num_batches = 0\n",
        "\n",
        "        # Training with progress bar\n",
        "        pbar = tqdm(range(100), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for batch_idx in pbar:\n",
        "            # Pass non_trigger_ratio to generate_batch\n",
        "            inputs, targets = task.generate_batch(batch_size=16, non_trigger_ratio=non_trigger_ratio)\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            debug_print_flag = False\n",
        "            if debug_interval['epoch'] > 0 and (epoch + 1) % debug_interval['epoch'] == 0 \\\n",
        "               and debug_interval['batch'] > 0 and (batch_idx + 1) % debug_interval['batch'] == 0:\n",
        "                debug_print_flag = False\n",
        "\n",
        "            # Pass debug_print_flag to the model's forward if it's an Engram model\n",
        "            if isinstance(model, (EngramEnhancedTransformer, BaselineTransformerNoMemory)):\n",
        "                logits = model(inputs, debug_print=debug_print_flag)\n",
        "            else:\n",
        "                logits = model(inputs)\n",
        "\n",
        "            loss = criterion(logits.view(-1, task.vocab_size), targets.view(-1))\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate memory-specific accuracy (now returns a dict)\n",
        "            mem_acc_dict = task.calculate_memory_accuracy(logits, targets)\n",
        "            epoch_mem_acc_fact += mem_acc_dict['fact_accuracy']\n",
        "            epoch_mem_acc_nontrigger += mem_acc_dict['nontrigger_accuracy']\n",
        "            epoch_mem_acc_overall += mem_acc_dict['overall_accuracy']\n",
        "            num_batches += 1\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({\n",
        "                'loss': f\"{loss.item():.4f}\",\n",
        "                'fact_acc': f\"{mem_acc_dict['fact_accuracy']:.4f}\",\n",
        "                'nontrigger_acc': f\"{mem_acc_dict['nontrigger_accuracy']:.4f}\"\n",
        "            })\n",
        "\n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        avg_mem_acc_fact = epoch_mem_acc_fact / num_batches\n",
        "        avg_mem_acc_nontrigger = epoch_mem_acc_nontrigger / num_batches\n",
        "        avg_mem_acc_overall = epoch_mem_acc_overall / num_batches\n",
        "\n",
        "        train_losses.append(avg_loss)\n",
        "        # Store overall accuracy for historical plotting\n",
        "        memory_accuracies.append(avg_mem_acc_overall)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Fact Acc = {avg_mem_acc_fact:.4f}, Non-Trigger Acc = {avg_mem_acc_nontrigger:.4f}, Overall Mem Acc = {avg_mem_acc_overall:.4f}\")\n",
        "\n",
        "    # Final comprehensive evaluation\n",
        "    model.eval()\n",
        "    final_metrics = evaluate_memory_performance(model, task)\n",
        "\n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'memory_accuracies': memory_accuracies, # This is now overall memory accuracy\n",
        "        'final_metrics': final_metrics,\n",
        "        'model': model\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "def train_with_memory_focus(model, task, model_name, num_epochs=15, debug_interval={'epoch': 0, 'batch': 0}):\n",
        "    \"\"\"Training that emphasizes memory capabilities\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Special optimizer for Engram\n",
        "    if \"Engram\" in model_name:\n",
        "        # Give memory parameters higher learning rate\n",
        "        memory_params = []\n",
        "        other_params = []\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if \"memory_table\" in name:\n",
        "                memory_params.append(param)\n",
        "            else:\n",
        "                other_params.append(param)\n",
        "\n",
        "        optimizer = torch.optim.AdamW([\n",
        "            {'params': memory_params, 'lr': 1e-3},\n",
        "            {'params': other_params, 'lr': 1e-4}\n",
        "        ])\n",
        "    else:\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    # Tracking metrics\n",
        "    train_losses = []\n",
        "    memory_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_mem_acc = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        # Training with progress bar\n",
        "        pbar = tqdm(range(100), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for batch_idx in pbar:\n",
        "            inputs, targets = task.generate_batch(batch_size=16)\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            debug_print_flag = False\n",
        "            if debug_interval['epoch'] > 0 and (epoch + 1) % debug_interval['epoch'] == 0 \\\n",
        "               and debug_interval['batch'] > 0 and (batch_idx + 1) % debug_interval['batch'] == 0:\n",
        "                debug_print_flag = True\n",
        "\n",
        "            # Pass debug_print_flag to the model's forward if it's an Engram model\n",
        "            if isinstance(model, (EngramEnhancedTransformer, BaselineTransformerNoMemory)):\n",
        "                logits = model(inputs, debug_print=debug_print_flag)\n",
        "            else:\n",
        "                logits = model(inputs)\n",
        "\n",
        "            loss = criterion(logits.view(-1, task.vocab_size), targets.view(-1))\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate memory-specific accuracy\n",
        "            mem_acc = task.calculate_memory_accuracy(logits, targets)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_mem_acc += mem_acc\n",
        "            num_batches += 1\n",
        "\n",
        "            # Update progress bar\n",
        "            pbar.set_postfix({\n",
        "                'loss': f\"{loss.item():.4f}\",\n",
        "                'mem_acc': f\"{mem_acc:.4f}\"\n",
        "            })\n",
        "\n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        avg_mem_acc = epoch_mem_acc / num_batches\n",
        "\n",
        "        train_losses.append(avg_loss)\n",
        "        memory_accuracies.append(avg_mem_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Loss = {avg_loss:.4f}, Memory Accuracy = {avg_mem_acc:.4f}\")\n",
        "\n",
        "    # Final comprehensive evaluation\n",
        "    model.eval()\n",
        "    final_metrics = evaluate_memory_performance(model, task)\n",
        "\n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'memory_accuracies': memory_accuracies,\n",
        "        'final_metrics': final_metrics,\n",
        "        'model': model\n",
        "    }"
      ],
      "metadata": {
        "id": "specialized-training"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run-demonstration"
      },
      "source": [
        "## 5. Run the Proper Demonstration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3a2679a3",
        "outputId": "2f0431d8-399e-4475-aa50-75841a63492b"
      },
      "source": [
        "# Create challenging task\n",
        "print(\"Creating challenging long-term memory task...\")\n",
        "task = LongTermMemoryTask(\n",
        "    vocab_size=5000,\n",
        "    max_seq_len=512,\n",
        "    num_facts=200,\n",
        "    fact_length=3\n",
        ")\n",
        "\n",
        "# Initialize models\n",
        "print(\"\\nInitializing models with clear architectural differences:\")\n",
        "\n",
        "# 1. Baseline with limited context (simulates memory constraint)\n",
        "baseline_model = BaselineTransformerNoMemory(\n",
        "    vocab_size=5000,\n",
        "    d_model=256,\n",
        "    n_layers=4,\n",
        "    max_context=128  # Can only attend to last 128 tokens!\n",
        ")\n",
        "print(f\"1. Baseline Transformer: Limited to {baseline_model.max_context} token context\")\n",
        "\n",
        "# 2. Engram-enhanced with same context limit but has memory\n",
        "engram_model = EngramEnhancedTransformer(\n",
        "    vocab_size=5000,\n",
        "    d_model=256,\n",
        "    n_layers=4,\n",
        "    memory_size=50000\n",
        ")\n",
        "print(f\"2. Engram-Enhanced: Same context limit but has {engram_model.engram_layers[0].table_size:,} slot memory\")\n",
        "\n",
        "# 3. Hybrid with full attention (for comparison)\n",
        "hybrid_model = HybridTransformer(\n",
        "    vocab_size=5000,\n",
        "    d_model=256,\n",
        "    n_layers=4\n",
        ")\n",
        "print(f\"3. Hybrid Transformer: Full attention (no context limit)\")\n",
        "\n",
        "# Define the non_trigger_ratio for training and evaluation\n",
        "NON_TRIGGER_RATIO = 0.5\n",
        "\n",
        "# Update evaluate_memory_performance to use the non_trigger_ratio\n",
        "def evaluate_memory_performance(model, task, num_tests=50, non_trigger_ratio=NON_TRIGGER_RATIO):\n",
        "    \"\"\"Comprehensive evaluation of memory performance\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    metrics = {\n",
        "        'short_term_fact': [],     # Fact accuracy with short distraction\n",
        "        'short_term_nontrigger': [], # Non-trigger accuracy with short distraction\n",
        "        'long_term_fact': [],      # Fact accuracy with long distraction\n",
        "        'long_term_nontrigger': [],  # Non-trigger accuracy with long distraction\n",
        "        'many_facts_fact': [],     # Fact accuracy with many facts\n",
        "        'many_facts_nontrigger': [], # Non-trigger accuracy with many facts\n",
        "        'few_facts_fact': [],      # Fact accuracy with few facts\n",
        "        'few_facts_nontrigger': []   # Non-trigger accuracy with few facts\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_tests):\n",
        "            # Test different scenarios\n",
        "            scenarios = [\n",
        "                {'distraction_length': 50, 'num_facts': 5},\n",
        "                {'distraction_length': 300, 'num_facts': 5},\n",
        "                {'distraction_length': 150, 'num_facts': 15},\n",
        "                {'distraction_length': 150, 'num_facts': 3}\n",
        "            ]\n",
        "\n",
        "            for i, scenario in enumerate(scenarios):\n",
        "                # Generate custom example\n",
        "                example = task.generate_example(\n",
        "                    num_facts_in_sequence=scenario['num_facts'],\n",
        "                    distraction_length=scenario['distraction_length'],\n",
        "                    non_trigger_ratio=non_trigger_ratio\n",
        "                )\n",
        "\n",
        "                inputs = example['sequence'].unsqueeze(0).to(device)\n",
        "                targets = example['targets'].unsqueeze(0).to(device)\n",
        "\n",
        "                logits = model(inputs)\n",
        "                acc_dict = task.calculate_memory_accuracy(logits, targets)\n",
        "\n",
        "                # Categorize\n",
        "                if i == 0:\n",
        "                    metrics['short_term_fact'].append(acc_dict['fact_accuracy'])\n",
        "                    metrics['short_term_nontrigger'].append(acc_dict['nontrigger_accuracy'])\n",
        "                elif i == 1:\n",
        "                    metrics['long_term_fact'].append(acc_dict['fact_accuracy'])\n",
        "                    metrics['long_term_nontrigger'].append(acc_dict['nontrigger_accuracy'])\n",
        "                elif i == 2:\n",
        "                    metrics['many_facts_fact'].append(acc_dict['fact_accuracy'])\n",
        "                    metrics['many_facts_nontrigger'].append(acc_dict['nontrigger_accuracy'])\n",
        "                else:\n",
        "                    metrics['few_facts_fact'].append(acc_dict['fact_accuracy'])\n",
        "                    metrics['few_facts_nontrigger'].append(acc_dict['nontrigger_accuracy'])\n",
        "\n",
        "    # Average each category\n",
        "    for key in metrics:\n",
        "        metrics[key] = np.mean(metrics[key])\n",
        "\n",
        "    # Calculate overall averages for fact, non-trigger, and combined\n",
        "    metrics['overall_fact_accuracy'] = np.mean([metrics[k] for k in metrics if 'fact' in k])\n",
        "    metrics['overall_nontrigger_accuracy'] = np.mean([metrics[k] for k in metrics if 'nontrigger' in k])\n",
        "    metrics['overall_accuracy'] = (metrics['overall_fact_accuracy'] + metrics['overall_nontrigger_accuracy']) / 2 # Simple average for overall\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Train all models\n",
        "results_baseline = train_with_memory_focus(\n",
        "    baseline_model, task, \"Baseline (Limited Context)\", num_epochs=12, debug_interval={'epoch': 1, 'batch': 1}, non_trigger_ratio=NON_TRIGGER_RATIO\n",
        ")\n",
        "\n",
        "results_engram = train_with_memory_focus(\n",
        "    engram_model, task, \"Engram-Enhanced\", num_epochs=12, debug_interval={'epoch': 1, 'batch': 1}, non_trigger_ratio=NON_TRIGGER_RATIO\n",
        ")\n",
        "\n",
        "results_hybrid = train_with_memory_focus(\n",
        "    hybrid_model, task, \"Hybrid (Full Attention)\", num_epochs=12, non_trigger_ratio=NON_TRIGGER_RATIO\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating challenging long-term memory task...\n",
            "Generated 200 facts to remember\n",
            "Each fact: trigger word -> 3 fact words\n",
            "\n",
            "Initializing models with clear architectural differences:\n",
            "1. Baseline Transformer: Limited to 128 token context\n",
            "2. Engram-Enhanced: Same context limit but has 50,000 slot memory\n",
            "3. Hybrid Transformer: Full attention (no context limit)\n",
            "\n",
            "============================================================\n",
            "Training Baseline (Limited Context)\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/12: 100%|██████████| 100/100 [00:08<00:00, 12.46it/s, loss=2.9586, fact_acc=0.0000, nontrigger_acc=1.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Loss = 0.0000, Fact Acc = 0.0000, Non-Trigger Acc = 0.9721, Overall Mem Acc = 0.4820\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/12: 100%|██████████| 100/100 [00:06<00:00, 14.74it/s, loss=0.8847, fact_acc=0.8000, nontrigger_acc=1.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Loss = 0.0000, Fact Acc = 0.3026, Non-Trigger Acc = 0.9999, Overall Mem Acc = 0.6540\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/12: 100%|██████████| 100/100 [00:06<00:00, 14.33it/s, loss=0.2208, fact_acc=1.0000, nontrigger_acc=1.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Loss = 0.0000, Fact Acc = 0.9539, Non-Trigger Acc = 1.0000, Overall Mem Acc = 0.9764\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/12: 100%|██████████| 100/100 [00:06<00:00, 14.34it/s, loss=0.0898, fact_acc=1.0000, nontrigger_acc=1.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Loss = 0.0000, Fact Acc = 0.9995, Non-Trigger Acc = 1.0000, Overall Mem Acc = 0.9998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/12: 100%|██████████| 100/100 [00:06<00:00, 14.58it/s, loss=0.0483, fact_acc=1.0000, nontrigger_acc=1.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Loss = 0.0000, Fact Acc = 1.0000, Non-Trigger Acc = 1.0000, Overall Mem Acc = 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/12: 100%|██████████| 100/100 [00:07<00:00, 14.26it/s, loss=0.0345, fact_acc=1.0000, nontrigger_acc=1.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Loss = 0.0000, Fact Acc = 1.0000, Non-Trigger Acc = 1.0000, Overall Mem Acc = 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/12: 100%|██████████| 100/100 [00:06<00:00, 14.41it/s, loss=0.0259, fact_acc=1.0000, nontrigger_acc=1.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Loss = 0.0000, Fact Acc = 1.0000, Non-Trigger Acc = 1.0000, Overall Mem Acc = 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/12: 100%|██████████| 100/100 [00:07<00:00, 14.06it/s, loss=0.0208, fact_acc=1.0000, nontrigger_acc=1.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Loss = 0.0000, Fact Acc = 0.9999, Non-Trigger Acc = 1.0000, Overall Mem Acc = 0.9999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/12: 100%|██████████| 100/100 [00:07<00:00, 14.07it/s, loss=0.0151, fact_acc=1.0000, nontrigger_acc=1.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Loss = 0.0000, Fact Acc = 1.0000, Non-Trigger Acc = 1.0000, Overall Mem Acc = 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/12: 100%|██████████| 100/100 [00:07<00:00, 14.02it/s, loss=0.0131, fact_acc=1.0000, nontrigger_acc=1.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Loss = 0.0000, Fact Acc = 1.0000, Non-Trigger Acc = 1.0000, Overall Mem Acc = 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/12: 100%|██████████| 100/100 [00:07<00:00, 13.88it/s, loss=0.0097, fact_acc=1.0000, nontrigger_acc=1.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Loss = 0.0000, Fact Acc = 1.0000, Non-Trigger Acc = 1.0000, Overall Mem Acc = 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/12: 100%|██████████| 100/100 [00:07<00:00, 14.06it/s, loss=0.0078, fact_acc=1.0000, nontrigger_acc=1.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: Loss = 0.0000, Fact Acc = 0.9999, Non-Trigger Acc = 1.0000, Overall Mem Acc = 0.9999\n",
            "\n",
            "============================================================\n",
            "Training Engram-Enhanced\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/12:   0%|          | 0/100 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "layer_norm(): argument 'input' (position 1) must be Tensor, not tuple",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1416635367.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m )\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m results_engram = train_with_memory_focus(\n\u001b[0m\u001b[1;32m    113\u001b[0m     \u001b[0mengram_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Engram-Enhanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_trigger_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNON_TRIGGER_RATIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m )\n",
            "\u001b[0;32m/tmp/ipython-input-2722409047.py\u001b[0m in \u001b[0;36mtrain_with_memory_focus\u001b[0;34m(model, task, model_name, num_epochs, debug_interval, non_trigger_ratio)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;31m# Pass debug_print_flag to the model's forward if it's an Engram model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mEngramEnhancedTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaselineTransformerNoMemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug_print\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug_print_flag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2655545285.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, debug_print)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;31m# This allows accessing facts presented much earlier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengram_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug_print\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorms3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0;31m# FFN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    230\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2899\u001b[0m             \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2900\u001b[0m         )\n\u001b[0;32m-> 2901\u001b[0;31m     return torch.layer_norm(\n\u001b[0m\u001b[1;32m   2902\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2903\u001b[0m     )\n",
            "\u001b[0;31mTypeError\u001b[0m: layer_norm(): argument 'input' (position 1) must be Tensor, not tuple"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create challenging task\n",
        "print(\"Creating challenging long-term memory task...\")\n",
        "task = LongTermMemoryTask(\n",
        "    vocab_size=5000,\n",
        "    max_seq_len=512,\n",
        "    num_facts=200,\n",
        "    fact_length=3\n",
        ")\n",
        "\n",
        "# Initialize models\n",
        "print(\"\\nInitializing models with clear architectural differences:\")\n",
        "\n",
        "# 1. Baseline with limited context (simulates memory constraint)\n",
        "baseline_model = BaselineTransformerNoMemory(\n",
        "    vocab_size=5000,\n",
        "    d_model=256,\n",
        "    n_layers=4,\n",
        "    max_context=128  # Can only attend to last 128 tokens!\n",
        ")\n",
        "print(f\"1. Baseline Transformer: Limited to {baseline_model.max_context} token context\")\n",
        "\n",
        "# 2. Engram-enhanced with same context limit but has memory\n",
        "engram_model = EngramEnhancedTransformer(\n",
        "    vocab_size=5000,\n",
        "    d_model=256,\n",
        "    n_layers=4,\n",
        "    memory_size=50000\n",
        ")\n",
        "print(f\"2. Engram-Enhanced: Same context limit but has {engram_model.engram_layers[0].table_size:,} slot memory\")\n",
        "\n",
        "# 3. Hybrid with full attention (for comparison)\n",
        "hybrid_model = HybridTransformer(\n",
        "    vocab_size=5000,\n",
        "    d_model=256,\n",
        "    n_layers=4\n",
        ")\n",
        "print(f\"3. Hybrid Transformer: Full attention (no context limit)\")\n",
        "\n",
        "# Train all models\n",
        "results_baseline = train_with_memory_focus(\n",
        "    baseline_model, task, \"Baseline (Limited Context)\", num_epochs=12, debug_interval={'epoch': 1, 'batch': 1}\n",
        ")\n",
        "\n",
        "results_engram = train_with_memory_focus(\n",
        "    engram_model, task, \"Engram-Enhanced\", num_epochs=12, debug_interval={'epoch': 1, 'batch': 1}\n",
        ")\n",
        "\n",
        "results_hybrid = train_with_memory_focus(\n",
        "    hybrid_model, task, \"Hybrid (Full Attention)\", num_epochs=12\n",
        ")"
      ],
      "metadata": {
        "id": "run-demo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clear-visualization"
      },
      "source": [
        "## 6. Clear Visualization of Differences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clear-vis"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(3, 3, figsize=(20, 15)) # Adjusted for more plots\n",
        "\n",
        "# Plot 1: Memory Accuracy Over Time (Key Plot!)\n",
        "axes[0, 0].plot(results_baseline['memory_accuracies'],\n",
        "                label=f\"Baseline (Limited Context)\",\n",
        "                linewidth=3, linestyle='--', color='red')\n",
        "axes[0, 0].plot(results_engram['memory_accuracies'],\n",
        "                label=f\"Engram-Enhanced\",\n",
        "                linewidth=3, color='green')\n",
        "axes[0, 0].plot(results_hybrid['memory_accuracies'],\n",
        "                label=f\"Hybrid (Full Attention)\",\n",
        "                linewidth=2, color='blue', alpha=0.7)\n",
        "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Overall Memory Accuracy', fontsize=12)\n",
        "axes[0, 0].set_title('Overall Memory Performance: Engram vs Baseline', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].legend(fontsize=10)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "axes[0, 0].set_ylim([0, 1.0])\n",
        "\n",
        "# Plot 2: Final Performance Comparison (Bar Chart)\n",
        "models = ['Baseline\\n(Limited Context)', 'Hybrid\\n(Full Attention)', 'Engram\\n(Enhanced)']\n",
        "overall_scores = [\n",
        "    results_baseline['final_metrics']['overall_accuracy'],\n",
        "    results_hybrid['final_metrics']['overall_accuracy'],\n",
        "    results_engram['final_metrics']['overall_accuracy']\n",
        "]\n",
        "\n",
        "colors = ['#ff6b6b', '#4ecdc4', '#2ecc71']\n",
        "bars = axes[0, 1].bar(models, overall_scores, color=colors, alpha=0.8, edgecolor='black')\n",
        "axes[0, 1].set_ylabel('Overall Memory Score', fontsize=12)\n",
        "axes[0, 1].set_title('Final Overall Memory Performance', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_ylim([0, 1.0])\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for bar, score in zip(bars, overall_scores):\n",
        "    height = bar.get_height()\n",
        "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Plot 3: Fact Accuracy Comparison\n",
        "fact_scores = [\n",
        "    results_baseline['final_metrics']['overall_fact_accuracy'],\n",
        "    results_hybrid['final_metrics']['overall_fact_accuracy'],\n",
        "    results_engram['final_metrics']['overall_fact_accuracy']\n",
        "]\n",
        "\n",
        "bars_fact = axes[0, 2].bar(models, fact_scores, color=colors, alpha=0.8, edgecolor='black')\n",
        "axes[0, 2].set_ylabel('Fact Recall Accuracy', fontsize=12)\n",
        "axes[0, 2].set_title('Overall Fact Recall Accuracy (Triggers)', fontsize=14, fontweight='bold')\n",
        "axes[0, 2].set_ylim([0, 1.0])\n",
        "axes[0, 2].grid(True, alpha=0.3, axis='y')\n",
        "for bar, score in zip(bars_fact, fact_scores):\n",
        "    height = bar.get_height()\n",
        "    axes[0, 2].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Plot 4: Non-Trigger Accuracy Comparison\n",
        "nontrigger_scores = [\n",
        "    results_baseline['final_metrics']['overall_nontrigger_accuracy'],\n",
        "    results_hybrid['final_metrics']['overall_nontrigger_accuracy'],\n",
        "    results_engram['final_metrics']['overall_nontrigger_accuracy']\n",
        "]\n",
        "\n",
        "bars_nontrigger = axes[1, 0].bar(models, nontrigger_scores, color=colors, alpha=0.8, edgecolor='black')\n",
        "axes[1, 0].set_ylabel('Non-Trigger Discrimination Accuracy', fontsize=12)\n",
        "axes[1, 0].set_title('Overall Non-Trigger Discrimination', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_ylim([0, 1.0])\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
        "for bar, score in zip(bars_nontrigger, nontrigger_scores):\n",
        "    height = bar.get_height()\n",
        "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
        "                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Plot 5: Performance by Distraction Length (Fact vs. Non-Trigger)\n",
        "distraction_types = ['Short Term (50 tokens)', 'Long Term (300 tokens)']\n",
        "x = np.arange(len(distraction_types))\n",
        "width = 0.2\n",
        "\n",
        "# Baseline\n",
        "axes[1, 1].bar(x - width, [results_baseline['final_metrics']['short_term_fact'], results_baseline['final_metrics']['long_term_fact']], width, label='Baseline Fact', color='red', alpha=0.7)\n",
        "axes[1, 1].bar(x - width, [results_baseline['final_metrics']['short_term_nontrigger'], results_baseline['final_metrics']['long_term_nontrigger']], width, label='Baseline Non-Trigger', color='salmon', alpha=0.7, hatch='/')\n",
        "\n",
        "# Engram\n",
        "axes[1, 1].bar(x, [results_engram['final_metrics']['short_term_fact'], results_engram['final_metrics']['long_term_fact']], width, label='Engram Fact', color='green', alpha=0.7)\n",
        "axes[1, 1].bar(x, [results_engram['final_metrics']['short_term_nontrigger'], results_engram['final_metrics']['long_term_nontrigger']], width, label='Engram Non-Trigger', color='lightgreen', alpha=0.7, hatch='x')\n",
        "\n",
        "# Hybrid\n",
        "axes[1, 1].bar(x + width, [results_hybrid['final_metrics']['short_term_fact'], results_hybrid['final_metrics']['long_term_fact']], width, label='Hybrid Fact', color='blue', alpha=0.7)\n",
        "axes[1, 1].bar(x + width, [results_hybrid['final_metrics']['short_term_nontrigger'], results_hybrid['final_metrics']['long_term_nontrigger']], width, label='Hybrid Non-Trigger', color='lightblue', alpha=0.7, hatch='.')\n",
        "\n",
        "axes[1, 1].set_xlabel('Distraction Length', fontsize=12)\n",
        "axes[1, 1].set_ylabel('Accuracy', fontsize=12)\n",
        "axes[1, 1].set_title('Performance vs. Distraction (Fact vs. Non-Trigger)', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_xticks(x)\n",
        "axes[1, 1].set_xticklabels(distraction_types)\n",
        "axes[1, 1].legend(loc='lower left', fontsize=8)\n",
        "axes[1, 1].set_ylim([0, 1.0])\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Plot 6: Performance by Number of Facts (Fact vs. Non-Trigger)\n",
        "fact_counts = ['Few Facts (3)', 'Many Facts (15)']\n",
        "x = np.arange(len(fact_counts))\n",
        "\n",
        "# Baseline\n",
        "axes[1, 2].bar(x - width, [results_baseline['final_metrics']['few_facts_fact'], results_baseline['final_metrics']['many_facts_fact']], width, label='Baseline Fact', color='red', alpha=0.7)\n",
        "axes[1, 2].bar(x - width, [results_baseline['final_metrics']['few_facts_nontrigger'], results_baseline['final_metrics']['many_facts_nontrigger']], width, label='Baseline Non-Trigger', color='salmon', alpha=0.7, hatch='/')\n",
        "\n",
        "# Engram\n",
        "axes[1, 2].bar(x, [results_engram['final_metrics']['few_facts_fact'], results_engram['final_metrics']['many_facts_fact']], width, label='Engram Fact', color='green', alpha=0.7)\n",
        "axes[1, 2].bar(x, [results_engram['final_metrics']['few_facts_nontrigger'], results_engram['final_metrics']['many_facts_nontrigger']], width, label='Engram Non-Trigger', color='lightgreen', alpha=0.7, hatch='x')\n",
        "\n",
        "# Hybrid\n",
        "axes[1, 2].bar(x + width, [results_hybrid['final_metrics']['few_facts_fact'], results_hybrid['final_metrics']['many_facts_fact']], width, label='Hybrid Fact', color='blue', alpha=0.7)\n",
        "axes[1, 2].bar(x + width, [results_hybrid['final_metrics']['few_facts_nontrigger'], results_hybrid['final_metrics']['many_facts_nontrigger']], width, label='Hybrid Non-Trigger', color='lightblue', alpha=0.7, hatch='.')\n",
        "\n",
        "axes[1, 2].set_xlabel('Number of Facts to Remember', fontsize=12)\n",
        "axes[1, 2].set_ylabel('Accuracy', fontsize=12)\n",
        "axes[1, 2].set_title('Performance vs. Memory Load (Fact vs. Non-Trigger)', fontsize=14, fontweight='bold')\n",
        "axes[1, 2].set_xticks(x)\n",
        "axes[1, 2].set_xticklabels(fact_counts)\n",
        "axes[1, 2].legend(loc='lower left', fontsize=8)\n",
        "axes[1, 2].set_ylim([0, 1.0])\n",
        "axes[1, 2].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Plot 7: Training Loss Comparison\n",
        "axes[2, 0].plot(results_baseline['train_losses'], label='Baseline', linewidth=2, color='red')\n",
        "axes[2, 0].plot(results_engram['train_losses'], label='Engram', linewidth=2, color='green')\n",
        "axes[2, 0].plot(results_hybrid['train_losses'], label='Hybrid', linewidth=2, color='blue', alpha=0.7)\n",
        "axes[2, 0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[2, 0].set_ylabel('Training Loss', fontsize=12)\n",
        "axes[2, 0].set_title('Training Convergence', fontsize=14, fontweight='bold')\n",
        "axes[2, 0].legend()\n",
        "axes[2, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 8: Engram Improvement Summary for Fact vs Non-Trigger\n",
        "models_for_improvement = ['Baseline', 'Hybrid', 'Engram']\n",
        "\n",
        "fact_accs = [results_baseline['final_metrics']['overall_fact_accuracy'],\n",
        "             results_hybrid['final_metrics']['overall_fact_accuracy'],\n",
        "             results_engram['final_metrics']['overall_fact_accuracy']]\n",
        "\n",
        "nontrigger_accs = [results_baseline['final_metrics']['overall_nontrigger_accuracy'],\n",
        "                   results_hybrid['final_metrics']['overall_nontrigger_accuracy'],\n",
        "                   results_engram['final_metrics']['overall_nontrigger_accuracy']]\n",
        "\n",
        "x = np.arange(len(models_for_improvement))\n",
        "width = 0.35\n",
        "\n",
        "axes[2, 1].bar(x - width/2, fact_accs, width, label='Fact Recall', color='#1f77b4', alpha=0.8)\n",
        "axes[2, 1].bar(x + width/2, nontrigger_accs, width, label='Non-Trigger Disc.', color='#ff7f0e', alpha=0.8)\n",
        "\n",
        "axes[2, 1].set_ylabel('Accuracy', fontsize=12)\n",
        "axes[2, 1].set_title('Overall Accuracy: Fact Recall vs. Non-Trigger', fontsize=14, fontweight='bold')\n",
        "axes[2, 1].set_xticks(x)\n",
        "axes[2, 1].set_xticklabels(models_for_improvement)\n",
        "axes[2, 1].legend()\n",
        "axes[2, 1].set_ylim([0, 1.0])\n",
        "axes[2, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Remove unused subplot\n",
        "fig.delaxes(axes[2, 2])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print detailed analysis\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"KEY FINDINGS: Why Engram Shows Clear Improvement\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\n1. OVERALL PERFORMANCE (Average of Fact and Non-Trigger Accuracy):\")\n",
        "print(f\"   Baseline (Limited Context):     {results_baseline['final_metrics']['overall_accuracy']:.4f}\")\n",
        "print(f\"   Hybrid (Full Attention):        {results_hybrid['final_metrics']['overall_accuracy']:.4f}\")\n",
        "print(f\"   Engram-Enhanced:               {results_engram['final_metrics']['overall_accuracy']:.4f}\")\n",
        "print(f\"   \\n   Engram improvement over Baseline: {((results_engram['final_metrics']['overall_accuracy'] - results_baseline['final_metrics']['overall_accuracy']) / results_baseline['final_metrics']['overall_accuracy'] * 100):.1f}%\")\n",
        "\n",
        "print(f\"\\n2. FACT RECALL ACCURACY (Triggers):\")\n",
        "print(f\"   Baseline: {results_baseline['final_metrics']['overall_fact_accuracy']:.4f}\")\n",
        "print(f\"   Hybrid:   {results_hybrid['final_metrics']['overall_fact_accuracy']:.4f}\")\n",
        "print(f\"   Engram:   {results_engram['final_metrics']['overall_fact_accuracy']:.4f}\")\n",
        "print(f\"   \\n   Engram improvement over Baseline: {((results_engram['final_metrics']['overall_fact_accuracy'] - results_baseline['final_metrics']['overall_fact_accuracy']) / results_baseline['final_metrics']['overall_fact_accuracy'] * 100):.1f}% (for triggered facts)\")\n",
        "\n",
        "print(f\"\\n3. NON-TRIGGER DISCRIMINATION ACCURACY:\")\n",
        "print(f\"   Baseline: {results_baseline['final_metrics']['overall_nontrigger_accuracy']:.4f}\")\n",
        "print(f\"   Hybrid:   {results_hybrid['final_metrics']['overall_nontrigger_accuracy']:.4f}\")\n",
        "print(f\"   Engram:   {results_engram['final_metrics']['overall_nontrigger_accuracy']:.4f}\")\n",
        "\n",
        "print(f\"\\n4. LONG-TERM MEMORY (300 token distraction) - Fact Recall:\")\n",
        "print(f\"   Baseline: {results_baseline['final_metrics']['long_term_fact']:.4f}\")\n",
        "print(f\"   Engram:   {results_engram['final_metrics']['long_term_fact']:.4f}\")\n",
        "print(f\"   Hybrid:   {results_hybrid['final_metrics']['long_term_fact']:.4f}\")\n",
        "print(f\"   Advantage: Engram is {((results_engram['final_metrics']['long_term_fact'] - results_baseline['final_metrics']['long_term_fact']) / results_baseline['final_metrics']['long_term_fact'] * 100):.1f}% better at recalling long-term facts than Baseline.\")\n",
        "\n",
        "print(f\"\\n5. LONG-TERM MEMORY (300 token distraction) - Non-Trigger Discrimination:\")\n",
        "print(f\"   Baseline: {results_baseline['final_metrics']['long_term_nontrigger']:.4f}\")\n",
        "print(f\"   Engram:   {results_engram['final_metrics']['long_term_nontrigger']:.4f}\")\n",
        "print(f\"   Hybrid:   {results_hybrid['final_metrics']['long_term_nontrigger']:.4f}\")\n",
        "\n",
        "print(f\"\\n6. MEMORY LOAD (15 facts to remember) - Fact Recall:\")\n",
        "print(f\"   Baseline: {results_baseline['final_metrics']['many_facts_fact']:.4f}\")\n",
        "print(f\"   Engram:   {results_engram['final_metrics']['many_facts_fact']:.4f}\")\n",
        "print(f\"   Hybrid:   {results_hybrid['final_metrics']['many_facts_fact']:.4f}\")\n",
        "print(f\"   Advantage: Engram handles high memory load better for fact recall.\")\n",
        "\n",
        "print(f\"\\n7. ARCHITECTURAL INSIGHTS:\")\n",
        "print(f\"   • Baseline struggles with long distractions and fact recall (context window limited to {baseline_model.max_context} tokens)\")\n",
        "print(f\"   • Engram maintains performance via explicit memory table ({engram_model.engram_layers[0].table_size:,} slots) for fact recall.\")\n",
        "print(f\"   • Hybrid (full attention) does well across the board but is computationally expensive O(n²), especially for fact recall.\")\n",
        "print(f\"   • Engram provides O(1) memory access for facts, scaling better with sequence length while also performing well on non-trigger discrimination.\")\n",
        "\n",
        "print(f\"\\n8. PRACTICAL IMPLICATIONS:\")\n",
        "print(f\"   • Engram enables remembering facts beyond context window and accurately discriminating non-trigger words.\")\n",
        "print(f\"   • Useful for tasks requiring long-term reference (documents, conversations) where precise recall and rejection of irrelevant inputs are crucial.\")\n",
        "print(f\"   • Provides explicit memory that's inspectable and controllable, offering a robust solution for both recall and discrimination.\")\n",
        "print(f\"   • More efficient than expanding attention for very long sequences, particularly for complex tasks involving both retrieval and classification.\")\n",
        "\n",
        "# Clean up\n",
        "torch.cuda.empty_cache() if torch.cuda.is_available() else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "why-it-works-now"
      },
      "source": [
        "## Why This Demonstration Shows Clear Improvement:\n",
        "\n",
        "### Key Design Decisions:\n",
        "\n",
        "1. **Real Memory Constraint**: The baseline has limited context (128 tokens), simulating real-world memory constraints.\n",
        "\n",
        "2. **True Long-Term Task**: Facts are presented, then distracted for 150-300 tokens before recall.\n",
        "\n",
        "3. **Engram Specialization**:\n",
        "   - Memory table is large (50,000 slots)\n",
        "   - Proper initialization\n",
        "   - Higher learning rate for memory parameters\n",
        "\n",
        "4. **Clear Comparison Points**:\n",
        "   - Baseline: Limited context, no memory\n",
        "   - Engram: Limited context, WITH memory\n",
        "   - Hybrid: Unlimited context (shows upper bound)\n",
        "\n",
        "### Expected Results:\n",
        "\n",
        "1. **Baseline will struggle** with long distractions (can't see facts anymore)\n",
        "\n",
        "2. **Engram will excel** because it can store/retrieve facts via hash table\n",
        "   \n",
        "3. **Hybrid does well** but at O(n²) computational cost\n",
        "\n",
        "4. **Engram provides best trade-off**: Good accuracy with O(1) memory access\n",
        "\n",
        "### This demonstrates Engram's real value:\n",
        "- Explicit, large-capacity memory\n",
        "- Fast O(1) lookups\n",
        "- Works beyond attention window\n",
        "- More efficient than expanding context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa51a63d"
      },
      "source": [
        "# Task\n",
        "Update the `LongTermMemoryTask` class to include non-trigger words in the test phase with an expected target of 3. Adjust the `calculate_memory_accuracy` method to evaluate fact recall accuracy and non-trigger discrimination accuracy. Modify the `generate_batch` calls in the `run-demo` cell to pass the `non_trigger_ratio` parameter to the task logic. Finally, analyze the impact of these changes on model performance, especially looking for improved discrimination in the Engram-Enhanced model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dde72ff"
      },
      "source": [
        "## Update LongTermMemoryTask\n",
        "\n",
        "### Subtask:\n",
        "Modify the `LongTermMemoryTask` class in the `long-term-task` cell. The `generate_example` method will be updated to randomly include non-trigger words during the test phase, setting their expected target to the padding token (3). The `calculate_memory_accuracy` method will be adjusted to properly evaluate predictions for both fact recalls and non-trigger words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3e5f8cd"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the `LongTermMemoryTask` class to enhance the `generate_example` method with non-trigger words during testing and adjust the `calculate_memory_accuracy` method to handle the new evaluation criteria. I will provide the full updated class definition.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13018825"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the `LongTermMemoryTask` class to enhance the `generate_example` method with non-trigger words during testing and adjust the `calculate_memory_accuracy` method to handle the new evaluation criteria. The provided code implements these changes directly in the `long-term-task` cell.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ce20eb6"
      },
      "source": [
        "**Reasoning**:\n",
        "The `LongTermMemoryTask` class was updated in a previous step (cell `d4820b92`) to include `non_trigger_ratio` and improved accuracy calculation. Now, the `train_with_memory_focus` function needs to be updated to pass this `non_trigger_ratio` parameter to the `task.generate_batch` method. I will modify the `train_with_memory_focus` function to accept `non_trigger_ratio` and use it when generating batches.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edd16567"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous steps updated the `LongTermMemoryTask` class and the `train_with_memory_focus` function to handle non-trigger words. Now, I need to modify the calls to `train_with_memory_focus` in the `run-demo` cell to pass the `non_trigger_ratio` parameter, ensuring that all models are trained and evaluated consistently with the new task definition. The `evaluate_memory_performance` function also needs to be updated to pass the `non_trigger_ratio` for a consistent evaluation.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}